In this paper, the authors present a new class of learning methods for support vector machines based on regularized kernels. These methods are particularly useful because they can be used to predict uncertainty in high-dimensionality situations. They show that an additive model can achieve better learning rates than a generalized linear model with a fixed loss function. This paper also discusses sparsity, which is one of the most important considerations in the literature when it comes to learning rate estimation. The first example uses a convex loss function and then a classical regularization term called "xmath1," which is regarded as a smoothing penalty but not by a sparcity penalty. The second example takes advantage of the fact that all the independent samples from the input space are identically distributed. The results of the first two examples are presented in order to illustrate the advantages of these additive models. The main goal of the paper is to prove that the learning rate provided by the proposed method is superior to that obtained by other approaches.