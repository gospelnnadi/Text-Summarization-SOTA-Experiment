additive models @xcite provide an important family of models for semiparametric regression or classification. many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0. in the last years many interesting results on learning rates have been published when the focus is on sparsity and when the classical least squares loss function is used, see e.g. @xcita. we present a new method that can provide a substantially better learning rate in high dimensions than an svm with a general kernel, say a classical gaussian rbf kernel, if the assumption of an additive model is satisfied. our leading example covers the learning rates for quantile regression based on the lipschitz continuous but non-differentiable pinball loss function, which is also called check function in the literature.