</s><s>additive models @xcite provide an important family of models for semiparametric regression or classification. Some of the most important reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared with fully nonparametric models. For example, learning rates of regularized kernel based methods based on a general convex and lipschitz continuous loss function typically suffer on bad statistical robustness properties, even if the kernel is bounded. In addition, the learning rate of a svm based on an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel, say a classical gaussian rbf kernel, if the assumption of an additive model is satisfied. In this paper, we present two examples of comparing additive models for learning rates with those from the literature. The first example deals with the learning rates given by theorem [ mainratesthm ] with the assumption that the additive model was valid. In the second example, we show how to compare additive models with product kernels. In the first example we show that the SVM based on the additive kernel has a significantly faster learning rate than the svm using the general kernel. In addition to this, we add some theoretical and numerical comparisons on the goodness of learning rates for additive models. We show how much better learning rates can be obtained by comparing additive model learning rates to those given by the literature. We also show how good learning rates are achieved by using the additive models. The learning rates shown in this paper depend on the ability of the Svm to provide the best learning rate for high dimensions. The learning rate shown in the paper depends on the accuracy of the model and on the performance of the machine. We then show how well the model can improve learning rates in high dimensionality. The best learning rates come from the ability to estimate learning rates. The most important learning rate is obtained by using a generalization model of the sobolev space, which is described in section [ proofsection ]. The most interesting learning rates we present here are those obtained by the use of a generalizing model. The generalization is that learning rates do not depend on accuracy, but rather on the capacity of the system. In other words, they do not rely on accuracy. In fact, they rely on the fact that the model is valid and that there is no need to worry about accuracy. This is because the model does not require accuracy.</s>