</s><s><s><s>In this paper, the authors evaluate the performance of two different kinds of machine learning algorithms: additive and non-additive. additive models are more flexible and more interpretable than fully nonparametric models. They also have a tendency to be less prone to the "curse of high dimensionality" than those based on linear or generalized linear models. Thus, our goal in this paper is to prove that an svm with an additive kernel can provide a substantially better learning rate in high dimensions than a general kernel over a limited space if the assumption that the model is based on an additive model is satisfied. The authors perform several experiments to determine the sensitivity and learning rate of their new methods. They use a loss function of the least squares solution to predict the likelihood of an outcome at a given point in time. Finally, they perform an experiment comparing the sensitivity of their additive model to that of a conventional Gaussian rbf kernel. The authors note that there is a technical difficulty in estimating the error due to the fact that the GRK has no direct connection to the marginal distribution, but they nevertheless obtain a good estimate of the error from several independent measures.</s>